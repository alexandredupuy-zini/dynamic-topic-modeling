{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-03 13:51:29,572 - kedro.io.data_catalog - INFO - Loading data from `raw_dataset` (CSVLocalDataSet)...\n",
      "2020-04-03 13:51:29,819 - kedro.io.data_catalog - INFO - Loading data from `dictionary` (DictionaryDataSet)...\n",
      "2020-04-03 13:51:29,820 - gensim.utils - INFO - loading Dictionary object from data/05_model_input/dictionary.dict\n",
      "2020-04-03 13:51:29,822 - gensim.utils - INFO - loaded data/05_model_input/dictionary.dict\n"
     ]
    }
   ],
   "source": [
    "dataset = catalog.load('raw_dataset')\n",
    "vocab = catalog.load('dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_sentence(docs):\n",
    "    tmp = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        splitted_doc = doc.split('.\\n')\n",
    "        for sd in splitted_doc:\n",
    "            sentences = sd.split('. ')\n",
    "            for s in sentences:\n",
    "                tmp.append(s)\n",
    "    return tmp\n",
    "\n",
    "def lowerize(docs):\n",
    "    # Convert to lowercase.\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = str(docs[idx]).lower()\n",
    "    return docs\n",
    "\n",
    "def tokenize(docs):\n",
    "    # Split into words.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])\n",
    "    return docs\n",
    "\n",
    "def remove_stop_words(docs):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = [w for w in docs[idx] if not w in stop_words]\n",
    "    return docs\n",
    "\n",
    "def remove_numbers(docs):\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    return docs\n",
    "\n",
    "def remove_word_with_length(docs, length=1):\n",
    "    # Remove words that are only (length=1) character.\n",
    "    docs = [[token for token in doc if len(token) > length] for doc in docs]\n",
    "    return docs\n",
    "\n",
    "def lemmatize(docs):\n",
    "    # Lemmatize the documents\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    return docs\n",
    "\n",
    "def add_bigram(docs, min_bigram_count=20):\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=min_bigram_count)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    return docs\n",
    "\n",
    "def remove_vocab(docs, vocab):\n",
    "    docs = np.array([[w for w in doc if w in vocab] for doc in docs])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting by sentence...\n",
      "\n",
      "Lowerizing...\n",
      "\n",
      "Tokenizing...\n",
      "\n",
      "Removing stop words...\n",
      "\n",
      "Removing unique numbers (not words that contain numbers)...\n",
      "\n",
      "Removing words that contain only one character...\n",
      "\n",
      "Lemmatizing...\n",
      "2020-04-03 13:51:45,336 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-03 13:51:45,518 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary(24357 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:45,685 - gensim.corpora.dictionary - INFO - adding document #20000 to Dictionary(32730 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:45,889 - gensim.corpora.dictionary - INFO - adding document #30000 to Dictionary(47754 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:46,062 - gensim.corpora.dictionary - INFO - adding document #40000 to Dictionary(52970 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:46,237 - gensim.corpora.dictionary - INFO - adding document #50000 to Dictionary(59272 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:46,398 - gensim.corpora.dictionary - INFO - adding document #60000 to Dictionary(64601 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:46,569 - gensim.corpora.dictionary - INFO - adding document #70000 to Dictionary(70685 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:46,744 - gensim.corpora.dictionary - INFO - adding document #80000 to Dictionary(78657 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:46,901 - gensim.corpora.dictionary - INFO - adding document #90000 to Dictionary(82266 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:47,078 - gensim.corpora.dictionary - INFO - adding document #100000 to Dictionary(88596 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:47,229 - gensim.corpora.dictionary - INFO - adding document #110000 to Dictionary(91425 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:47,391 - gensim.corpora.dictionary - INFO - adding document #120000 to Dictionary(94625 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:47,562 - gensim.corpora.dictionary - INFO - adding document #130000 to Dictionary(98944 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:47,726 - gensim.corpora.dictionary - INFO - adding document #140000 to Dictionary(101212 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:47,890 - gensim.corpora.dictionary - INFO - adding document #150000 to Dictionary(105007 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:48,068 - gensim.corpora.dictionary - INFO - adding document #160000 to Dictionary(112803 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:48,228 - gensim.corpora.dictionary - INFO - adding document #170000 to Dictionary(115840 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:48,389 - gensim.corpora.dictionary - INFO - adding document #180000 to Dictionary(118671 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:48,544 - gensim.corpora.dictionary - INFO - adding document #190000 to Dictionary(120941 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:48,741 - gensim.corpora.dictionary - INFO - adding document #200000 to Dictionary(136274 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:48,901 - gensim.corpora.dictionary - INFO - adding document #210000 to Dictionary(138609 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:49,070 - gensim.corpora.dictionary - INFO - adding document #220000 to Dictionary(140877 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:49,231 - gensim.corpora.dictionary - INFO - adding document #230000 to Dictionary(142893 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:49,393 - gensim.corpora.dictionary - INFO - adding document #240000 to Dictionary(144919 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:49,555 - gensim.corpora.dictionary - INFO - adding document #250000 to Dictionary(146833 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:49,717 - gensim.corpora.dictionary - INFO - adding document #260000 to Dictionary(148610 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "2020-04-03 13:51:49,875 - gensim.corpora.dictionary - INFO - built Dictionary(152643 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...) from 269709 documents (total 3302129 corpus positions)\n",
      "2020-04-03 13:51:50,034 - gensim.corpora.dictionary - INFO - discarding 148441 tokens: [('enlighten', 42), ('lerxst', 7), ('rac3', 13), ('60', 23), ('70', 39), ('bricklin', 16), ('bumper', 55), ('funky', 8), ('tellme', 2), ('neighborhood', 87)]...\n",
      "2020-04-03 13:51:50,035 - gensim.corpora.dictionary - INFO - keeping 4202 tokens which were in no less than 100 and no more than 188796 (=70.0%) documents\n",
      "2020-04-03 13:51:50,089 - gensim.corpora.dictionary - INFO - resulting dictionary: Dictionary(4202 unique tokens: ['anyone', 'car', 'college', 'could', 'day']...)\n",
      "Number of sentences: 269709\n",
      "Number of unique words: 4202\n"
     ]
    }
   ],
   "source": [
    "min_bigram_count = 20\n",
    "length = 1\n",
    "no_below = 100\n",
    "no_above = 0.70\n",
    "\n",
    "docs = dataset['text'].values\n",
    "\n",
    "print('\\nSplitting by sentence...')\n",
    "docs = split_by_sentence(docs)\n",
    "\n",
    "print('\\nLowerizing...')\n",
    "docs = lowerize(docs)\n",
    "\n",
    "print('\\nTokenizing...')\n",
    "docs = tokenize(docs)\n",
    "\n",
    "#print('\\nAdding bigrams...')\n",
    "#docs = add_bigram(docs, min_bigram_count=min_bigram_count)\n",
    "    \n",
    "print('\\nRemoving stop words...')\n",
    "docs = remove_stop_words(docs)\n",
    "\n",
    "print('\\nRemoving unique numbers (not words that contain numbers)...')\n",
    "docs = remove_numbers(docs)\n",
    "\n",
    "print('\\nRemoving words that contain only one character...')\n",
    "docs = remove_word_with_length(docs, length=length)\n",
    "\n",
    "print('\\nLemmatizing...')\n",
    "docs = lemmatize(docs)\n",
    "\n",
    "vocab = Dictionary(docs)\n",
    "vocab.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "\n",
    "docs = remove_vocab(docs, list(vocab.token2id))\n",
    "\n",
    "print('Number of sentences:', len(docs))\n",
    "print('Number of unique words:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train doc embeddings (gensim doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(docs):\n",
    "    for i, text in enumerate(docs):\n",
    "        yield TaggedDocument(text, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(read_corpus(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-03 13:54:37,989 - gensim.models.base_any2vec - WARNING - consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(vector_size=25, min_count=2, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-03 13:54:38,083 - gensim.models.doc2vec - INFO - collecting all words and their counts\n",
      "2020-04-03 13:54:38,086 - gensim.models.doc2vec - INFO - PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-04-03 13:54:38,140 - gensim.models.doc2vec - INFO - PROGRESS: at example #10000, processed 88269 words (1713446/s), 4135 word types, 10000 tags\n",
      "2020-04-03 13:54:38,164 - gensim.models.doc2vec - INFO - PROGRESS: at example #20000, processed 170352 words (3592848/s), 4190 word types, 20000 tags\n",
      "2020-04-03 13:54:38,189 - gensim.models.doc2vec - INFO - PROGRESS: at example #30000, processed 262545 words (3808046/s), 4200 word types, 30000 tags\n",
      "2020-04-03 13:54:38,213 - gensim.models.doc2vec - INFO - PROGRESS: at example #40000, processed 349396 words (3618515/s), 4200 word types, 40000 tags\n",
      "2020-04-03 13:54:38,236 - gensim.models.doc2vec - INFO - PROGRESS: at example #50000, processed 438275 words (3985932/s), 4200 word types, 50000 tags\n",
      "2020-04-03 13:54:38,260 - gensim.models.doc2vec - INFO - PROGRESS: at example #60000, processed 525863 words (3814833/s), 4200 word types, 60000 tags\n",
      "2020-04-03 13:54:38,284 - gensim.models.doc2vec - INFO - PROGRESS: at example #70000, processed 618136 words (3907273/s), 4200 word types, 70000 tags\n",
      "2020-04-03 13:54:38,309 - gensim.models.doc2vec - INFO - PROGRESS: at example #80000, processed 707107 words (3691690/s), 4201 word types, 80000 tags\n",
      "2020-04-03 13:54:38,336 - gensim.models.doc2vec - INFO - PROGRESS: at example #90000, processed 794204 words (3409562/s), 4201 word types, 90000 tags\n",
      "2020-04-03 13:54:38,362 - gensim.models.doc2vec - INFO - PROGRESS: at example #100000, processed 879054 words (3334380/s), 4201 word types, 100000 tags\n",
      "2020-04-03 13:54:38,385 - gensim.models.doc2vec - INFO - PROGRESS: at example #110000, processed 961108 words (3726263/s), 4201 word types, 110000 tags\n",
      "2020-04-03 13:54:38,409 - gensim.models.doc2vec - INFO - PROGRESS: at example #120000, processed 1052200 words (3865581/s), 4201 word types, 120000 tags\n",
      "2020-04-03 13:54:38,433 - gensim.models.doc2vec - INFO - PROGRESS: at example #130000, processed 1141862 words (3907964/s), 4202 word types, 130000 tags\n",
      "2020-04-03 13:54:38,457 - gensim.models.doc2vec - INFO - PROGRESS: at example #140000, processed 1229378 words (3867426/s), 4202 word types, 140000 tags\n",
      "2020-04-03 13:54:38,481 - gensim.models.doc2vec - INFO - PROGRESS: at example #150000, processed 1314120 words (3687037/s), 4202 word types, 150000 tags\n",
      "2020-04-03 13:54:38,504 - gensim.models.doc2vec - INFO - PROGRESS: at example #160000, processed 1403100 words (3968689/s), 4202 word types, 160000 tags\n",
      "2020-04-03 13:54:38,529 - gensim.models.doc2vec - INFO - PROGRESS: at example #170000, processed 1491424 words (3552801/s), 4202 word types, 170000 tags\n",
      "2020-04-03 13:54:38,553 - gensim.models.doc2vec - INFO - PROGRESS: at example #180000, processed 1577220 words (3703029/s), 4202 word types, 180000 tags\n",
      "2020-04-03 13:54:38,578 - gensim.models.doc2vec - INFO - PROGRESS: at example #190000, processed 1659774 words (3405665/s), 4202 word types, 190000 tags\n",
      "2020-04-03 13:54:38,606 - gensim.models.doc2vec - INFO - PROGRESS: at example #200000, processed 1748349 words (3317638/s), 4202 word types, 200000 tags\n",
      "2020-04-03 13:54:38,631 - gensim.models.doc2vec - INFO - PROGRESS: at example #210000, processed 1835286 words (3656094/s), 4202 word types, 210000 tags\n",
      "2020-04-03 13:54:38,658 - gensim.models.doc2vec - INFO - PROGRESS: at example #220000, processed 1924204 words (3395353/s), 4202 word types, 220000 tags\n",
      "2020-04-03 13:54:38,685 - gensim.models.doc2vec - INFO - PROGRESS: at example #230000, processed 2011901 words (3345509/s), 4202 word types, 230000 tags\n",
      "2020-04-03 13:54:38,709 - gensim.models.doc2vec - INFO - PROGRESS: at example #240000, processed 2098946 words (3781018/s), 4202 word types, 240000 tags\n",
      "2020-04-03 13:54:38,737 - gensim.models.doc2vec - INFO - PROGRESS: at example #250000, processed 2188187 words (3265772/s), 4202 word types, 250000 tags\n",
      "2020-04-03 13:54:38,760 - gensim.models.doc2vec - INFO - PROGRESS: at example #260000, processed 2276016 words (3877471/s), 4202 word types, 260000 tags\n",
      "2020-04-03 13:54:38,786 - gensim.models.doc2vec - INFO - collected 4202 word types and 269709 unique tags from a corpus of 269709 examples and 2356350 words\n",
      "2020-04-03 13:54:38,787 - gensim.models.word2vec - INFO - Loading a fresh vocabulary\n",
      "2020-04-03 13:54:38,794 - gensim.models.word2vec - INFO - effective_min_count=2 retains 4202 unique words (100% of original 4202, drops 0)\n",
      "2020-04-03 13:54:38,795 - gensim.models.word2vec - INFO - effective_min_count=2 leaves 2356350 word corpus (100% of original 2356350, drops 0)\n",
      "2020-04-03 13:54:38,804 - gensim.models.word2vec - INFO - deleting the raw counts dictionary of 4202 items\n",
      "2020-04-03 13:54:38,805 - gensim.models.word2vec - INFO - sample=0.001 downsamples 25 most-common words\n",
      "2020-04-03 13:54:38,805 - gensim.models.word2vec - INFO - downsampling leaves estimated 2242339 word corpus (95.2% of prior 2356350)\n",
      "2020-04-03 13:54:38,815 - gensim.models.base_any2vec - INFO - estimated required memory for 4202 words and 25 dimensions: 29912300 bytes\n",
      "2020-04-03 13:54:38,816 - gensim.models.word2vec - INFO - resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-03 13:55:21,439 - gensim.models.base_any2vec - INFO - training model with 3 workers on 4202 vocabulary and 25 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-04-03 13:55:22,542 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 5.65% examples, 126975 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:23,597 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 11.88% examples, 138744 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:24,721 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 18.19% examples, 139925 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-03 13:55:25,770 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 24.47% examples, 143035 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:26,791 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 30.59% examples, 145536 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:27,847 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 37.08% examples, 146543 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-03 13:55:28,863 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 43.14% examples, 146672 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-03 13:55:29,896 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 49.32% examples, 147599 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:30,965 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 55.81% examples, 147824 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:31,969 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 62.07% examples, 148847 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:33,006 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 68.56% examples, 149344 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:34,032 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 74.55% examples, 149001 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:35,097 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 80.87% examples, 149068 words/s, in_qsize 5, out_qsize 0\n",
      "2020-04-03 13:55:36,159 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 87.22% examples, 149154 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:37,206 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 93.10% examples, 148691 words/s, in_qsize 6, out_qsize 0\n",
      "2020-04-03 13:55:38,274 - gensim.models.base_any2vec - INFO - EPOCH 1 - PROGRESS: at 99.14% examples, 148121 words/s, in_qsize 2, out_qsize 1\n",
      "2020-04-03 13:55:38,277 - gensim.models.base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-03 13:55:38,287 - gensim.models.base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-03 13:55:38,298 - gensim.models.base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-03 13:55:38,299 - gensim.models.base_any2vec - INFO - EPOCH - 1 : training on 2356350 raw words (2512192 effective words) took 16.9s, 149091 effective words/s\n",
      "2020-04-03 13:55:38,300 - gensim.models.base_any2vec - INFO - training on a 2356350 raw words (2512192 effective words) took 16.9s, 149005 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynamicTopicModeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
