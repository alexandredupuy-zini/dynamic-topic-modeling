{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-03 20:17:15,701 - root - INFO - ** Kedro project Dynamic Topic Modeling\n",
      "2020-03-03 20:17:15,702 - root - INFO - Defined global variable `context` and `catalog`\n"
     ]
    }
   ],
   "source": [
    "%reload_kedro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing inertia to improve $k$ with the elbow rule is time consuming and can't really get automated. Increasing $k$ according to the data needs to be done differently. We will focus on dynamic method with a fixed $k$ for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-03 20:17:16,204 - kedro.io.data_catalog - INFO - Loading data from `UN_dictionary` (DictionaryDataSet)...\n"
     ]
    }
   ],
   "source": [
    "dictionary = catalog.load(\"UN_dictionary\")\n",
    "id2token = {v: k for k, v in dictionary.token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-03 20:17:16,281 - kedro.io.data_catalog - INFO - Loading data from `BOW_train` (ScipySparseMatrix)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5256x12459 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2965958 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW_train=catalog.load(\"BOW_train\")\n",
    "BOW_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-03 20:17:16,400 - kedro.io.data_catalog - INFO - Loading data from `timestamp_train` (CSVLocalDataSet)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeslice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timeslice\n",
       "0         20\n",
       "1         24\n",
       "2         23\n",
       "3         29\n",
       "4         39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_train = catalog.load(\"timestamp_train\")\n",
    "timestamp_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more interpretability, we have disabed bigrams, which weren't very valuable in the outputs of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer().fit_transform(BOW_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def top_terms(num_clusters, n_terms, id2token, order_centroids):\n",
    "    top_terms = []*num_clusters\n",
    "    for i in range(num_clusters):\n",
    "        cluster_terms = []\n",
    "        for ind in order_centroids[i,:n_terms]:\n",
    "            cluster_terms.append(id2token[ind])\n",
    "        top_terms.append(cluster_terms)\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:07<00:00,  5.98it/s]\n"
     ]
    }
   ],
   "source": [
    "n_terms = 5\n",
    "num_clusters = 10\n",
    "\n",
    "cluster_terms = []\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_jobs=-1)\n",
    "km.fit(tfidf[timestamp_train[timestamp_train['timeslice'] == 0].index])\n",
    "centroids = km.cluster_centers_\n",
    "order_centroids = centroids.argsort()[:,::-1]\n",
    "\n",
    "cluster_terms.append(top_terms(num_clusters, n_terms, id2token, order_centroids))\n",
    "\n",
    "for i in tqdm(range(1,timestamp_train.max().values[0] + 1)):\n",
    "    km = KMeans(n_clusters=num_clusters, n_jobs=-1, init=centroids, n_init=1)\n",
    "    km.fit(tfidf[timestamp_train[timestamp_train['timeslice'] == i].index])\n",
    "    centroids = km.cluster_centers_\n",
    "    order_centroids = centroids.argsort()[:,::-1]\n",
    "    \n",
    "    cluster_terms.append(top_terms(num_clusters, n_terms, id2token, order_centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accra', 'flourishing', 'subsidize', 'hundred', 'reconciliation']\n",
      "['monarchy', 'universality', 'affliction', 'smile', 'indignant']\n",
      "['jose', 'urbanization', 'thirteen', 'tum', 'incapacity']\n",
      "['vietnamese', 'advent', 'deficient', 'indefinite', 'stifled']\n",
      "['distributed', 'credibly', 'versus', 'hesitant', 'flourishing']\n",
      "['magic', 'buy', 'receded', 'investment', 'replaces']\n",
      "['unfpa', 'chronic', 'villager', 'adventurous', 'visa']\n",
      "['immunization', 'august', 'abort', 'tactic', 'contrary']\n",
      "['immunization', 'weaponry', 'checking', 'august', 'strengthened']\n",
      "['prevails', 'righteousness', 'malnourished', 'wrath', 'wiping']\n",
      "['magic', 'righteousness', 'malnourished', 'wrath', 'drive']\n",
      "['inequality', 'righteousness', 'malnourished', 'magic', 'persists']\n",
      "['wiping', 'malnourished', 'persists', 'righteousness', 'magic']\n",
      "['persists', 'wiping', 'affliction', 'righteousness', 'malnourished']\n",
      "['revitalisation', 'midterm', 'collaborating', 'troubled', 'cannon']\n",
      "['monarchy', 'foolish', 'highly', 'scientist', 'reply']\n",
      "['brought', 'disinherited', 'rein', 'inform', 'hook']\n",
      "['boycotted', 'engineering', 'perpetrator', 'magic', 'elude']\n",
      "['field', 'senegal', 'reopen', 'grappling', 'ki']\n",
      "['field', 'shackled', 'africanist', 'rein', 'accede']\n",
      "['frequency', 'expeditiously', 'negligence', 'tends', 'garba']\n",
      "['socialist', 'final', 'delay', 'taiwan', 'ostensible']\n",
      "['socialist', 'final', 'buenos', 'delay', 'proactive']\n",
      "['socialist', 'final', 'delay', 'kilowatt', 'buenos']\n",
      "['socialist', 'proactive', 'final', 'deliberating', 'normality']\n",
      "['socialist', 'delay', 'connectivity', 'final', 'buenos']\n",
      "['dissolve', 'propelled', 'swayed', 'applauded', 'asserting']\n",
      "['dissolve', 'propelled', 'exacerbation', 'heartened', 'lakhdar']\n",
      "['dissolve', 'unbridled', 'propelled', 'regarded', 'knesset']\n",
      "['dissolve', 'knesset', 'determining', 'springboard', 'senate']\n",
      "['carbon', 'concocted', 'questionable', 'regarding', 'peacekeeping']\n",
      "['carbon', 'regarding', 'gentleman', 'peacekeeping', 'knesset']\n",
      "['adopting', 'incarcerated', 'investment', 'misguided', 'jamahiriya']\n",
      "['investment', 'adopting', 'underlie', 'misguided', 'jamahiriya']\n",
      "['investment', 'reserved', 'underlie', 'progressive', 'adopting']\n",
      "['investment', 'progressive', 'adopting', 'urban', 'crossing']\n",
      "['focusing', 'finality', 'comer', 'savage', 'exhausting']\n",
      "['troublesome', 'attaching', 'stave', 'cultural', 'onion']\n",
      "['troublesome', 'attaching', 'stave', 'rotating', 'incomparable']\n",
      "['troublesome', 'attaching', 'outweigh', 'scenario', 'nixon']\n",
      "['troublesome', 'attaching', 'heap', 'incomparable', 'normalize']\n",
      "['candidly', 'alike', 'paraguayan', 'julius', 'deceptive']\n",
      "['blessed', 'assemble', 'allowed', 'enabling', 'risk']\n",
      "['blessed', 'assemble', 'racialist', 'risk', 'archaic']\n"
     ]
    }
   ],
   "source": [
    "for i in range(44):\n",
    "    print(cluster_terms[i][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering year by year isn't effective, as the previous years have the same weight as a single document of the current year. The clusters change too drastically and don't have any link from a year to another.\n",
    "\n",
    "Instead, we add years one by one until all the data is included, while keeping the means initialized where the previous clusters were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = 5\n",
    "num_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [15:50<00:00, 21.12s/it]\n"
     ]
    }
   ],
   "source": [
    "cluster_terms = []\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_jobs=-1)\n",
    "km.fit(tfidf[timestamp_train[timestamp_train['timeslice'] == 0].index])\n",
    "centroids = km.cluster_centers_\n",
    "order_centroids = centroids.argsort()[:,::-1]\n",
    "\n",
    "cluster_terms.append(top_terms(num_clusters, n_terms, id2token, order_centroids))\n",
    "\n",
    "for i in tqdm(range(1,timestamp_train.max().values[0] + 1)):\n",
    "    km = KMeans(n_clusters=num_clusters, n_jobs=-1, init=centroids, n_init=1)\n",
    "    km.fit(tfidf[timestamp_train[timestamp_train['timeslice'] <= i].index])\n",
    "    centroids = km.cluster_centers_\n",
    "    order_centroids = centroids.argsort()[:,::-1]\n",
    "    \n",
    "    cluster_terms.append(top_terms(num_clusters, n_terms, id2token, order_centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cluster_terms, open( \"all_nogram.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_terms = pickle.load(open( \"all_nogram.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exploitative', 'limitation', 'pooling', 'suggestion', 'writ']\n",
      "['affliction', 'flourishing', 'catalogue', 'limitation', 'inf']\n",
      "['flourishing', 'affliction', 'catalogue', 'malnourished', 'inf']\n",
      "['flourishing', 'affliction', 'fool', 'catalogue', 'proactive']\n",
      "['flourishing', 'fool', 'affliction', 'catalogue', 'limitation']\n",
      "['flourishing', 'fool', 'inf', 'proactive', 'calmly']\n",
      "['flourishing', 'fool', 'inf', 'calmly', 'tension']\n",
      "['flourishing', 'inf', 'fool', 'calmly', 'persuasion']\n",
      "['flourishing', 'inf', 'fool', 'calmly', 'persuasion']\n",
      "['flourishing', 'inf', 'fool', 'calmly', 'persuasion']\n",
      "['flourishing', 'inf', 'fool', 'calmly', 'ethically']\n",
      "['flourishing', 'inf', 'fool', 'calmly', 'ethically']\n",
      "['flourishing', 'inf', 'fool', 'calmly', 'persuasion']\n",
      "['flourishing', 'inf', 'fool', 'calmly', 'ethically']\n",
      "['flourishing', 'inf', 'ethically', 'fool', 'calmly']\n",
      "['flourishing', 'inf', 'ethically', 'calmly', 'fool']\n",
      "['flourishing', 'inf', 'ethically', 'calmly', 'proactive']\n",
      "['flourishing', 'inf', 'ethically', 'proactive', 'calmly']\n",
      "['flourishing', 'inf', 'ethically', 'proactive', 'grappling']\n",
      "['flourishing', 'inf', 'ethically', 'proactive', 'persuasion']\n",
      "['flourishing', 'inf', 'proactive', 'ethically', 'persuasion']\n",
      "['flourishing', 'inf', 'proactive', 'ethically', 'liberty']\n",
      "['flourishing', 'inf', 'proactive', 'ethically', 'liberty']\n",
      "['flourishing', 'inf', 'proactive', 'axiom', 'kilowatt']\n",
      "['flourishing', 'proactive', 'inf', 'deliberating', 'axiom']\n",
      "['flourishing', 'proactive', 'deliberating', 'inf', 'axiom']\n",
      "['flourishing', 'proactive', 'deliberating', 'inf', 'axiom']\n",
      "['flourishing', 'proactive', 'deliberating', 'inf', 'axiom']\n",
      "['proactive', 'deliberating', 'vorster', 'axiom', 'suggestion']\n",
      "['deliberating', 'proactive', 'vorster', 'suggestion', 'axiom']\n",
      "['vorster', 'proactive', 'deliberating', 'clarified', 'suggestion']\n",
      "['vorster', 'clarified', 'proactive', 'ally', 'deliberating']\n",
      "['peacekeeping', 'vorster', 'clarified', 'ally', 'kilowatt']\n",
      "['peacekeeping', 'vorster', 'ally', 'clarified', 'kilowatt']\n",
      "['peacekeeping', 'vorster', 'clarified', 'ally', 'redoubled']\n",
      "['peacekeeping', 'vorster', 'ally', 'clarified', 'launching']\n",
      "['peacekeeping', 'vorster', 'ally', 'launching', 'sweep']\n",
      "['peacekeeping', 'vorster', 'sweep', 'ally', 'launching']\n",
      "['peacekeeping', 'vorster', 'sweep', 'ally', 'clarified']\n",
      "['peacekeeping', 'vorster', 'sweep', 'ally', 'clarified']\n",
      "['peacekeeping', 'sweep', 'vorster', 'ally', 'clarified']\n",
      "['peacekeeping', 'sweep', 'vorster', 'clarified', 'ally']\n",
      "['peacekeeping', 'vorster', 'reactive', 'sweep', 'clarified']\n",
      "['peacekeeping', 'reactive', 'sweep', 'vorster', 'clarified']\n"
     ]
    }
   ],
   "source": [
    "for i in range(44):\n",
    "    print(cluster_terms[i][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method seems to detect strong changes but isn't very flexible. Let's use a slinding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inbetween(i, timestamp_train, w_size):\n",
    "    return timestamp_train[timestamp_train['timeslice'] <= i][timestamp_train['timeslice'] > i-w_size].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [05:33<00:00,  7.41s/it]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "w_size = 10\n",
    "\n",
    "cluster_terms = []\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_jobs=-1)\n",
    "km.fit(tfidf[timestamp_train[timestamp_train['timeslice'] == 0].index])\n",
    "centroids = km.cluster_centers_\n",
    "order_centroids = centroids.argsort()[:,::-1]\n",
    "\n",
    "cluster_terms.append(top_terms(num_clusters, n_terms, id2token, order_centroids))\n",
    "\n",
    "for i in tqdm(range(1,timestamp_train.max().values[0] + 1)):\n",
    "    km = KMeans(n_clusters=num_clusters, n_jobs=-1, init=centroids, n_init=1)\n",
    "    if i>w_size-1:\n",
    "        km.fit(tfidf[inbetween(i, timestamp_train, w_size)])\n",
    "    else:\n",
    "        km.fit(tfidf[timestamp_train[timestamp_train['timeslice'] <= i].index])\n",
    "    centroids = km.cluster_centers_\n",
    "    order_centroids = centroids.argsort()[:,::-1]\n",
    "    \n",
    "    cluster_terms.append(top_terms(num_clusters, n_terms, id2token, order_centroids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For w_size=5, we get the following clusters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cluster_terms, open( \"wsize5_nogram.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_terms = pickle.load(open( \"wsize5_nogram.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exploitative', 'enforceable', 'representativity', 'astonishing', 'flourishing']\n",
      "['exploitative', 'enforceable', 'astonishing', 'representativity', 'flourishing']\n",
      "['astonishing', 'exploitative', 'enforceable', 'representativity', 'flourishing']\n",
      "['astonishing', 'exploitative', 'enforceable', 'representativity', 'flourishing']\n",
      "['astonishing', 'exploitative', 'enforceable', 'representativity', 'flourishing']\n",
      "['astonishing', 'exploitative', 'enforceable', 'flourishing', 'representativity']\n",
      "['astonishing', 'exploitative', 'flourishing', 'enforceable', 'representativity']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'exploitative']\n",
      "['astonishing', 'enforceable', 'representativity', 'exploitative', 'flourishing']\n",
      "['astonishing', 'enforceable', 'representativity', 'exploitative', 'flourishing']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'indictment']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'ethically']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'ethically']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'ethically']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'ethically']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'ethically']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'ethically']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'ethically']\n",
      "['astonishing', 'enforceable', 'representativity', 'flourishing', 'followed']\n",
      "['astonishing', 'enforceable', 'representativity', 'followed', 'flourishing']\n",
      "['astonishing', 'enforceable', 'representativity', 'followed', 'flourishing']\n",
      "['astonishing', 'enforceable', 'representativity', 'followed', 'flourishing']\n",
      "['astonishing', 'enforceable', 'representativity', 'followed', 'lord']\n",
      "['astonishing', 'enforceable', 'representativity', 'followed', 'lord']\n",
      "['enforceable', 'representativity', 'astonishing', 'followed', 'bangladesh']\n",
      "['enforceable', 'representativity', 'astonishing', 'followed', 'unlock']\n",
      "['enforceable', 'representativity', 'unlock', 'cease', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'cease', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'ally', 'site']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'site', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'battle', 'site']\n",
      "['enforceable', 'representativity', 'unlock', 'battle', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'battle', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'battle', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'battle', 'ally']\n",
      "['enforceable', 'representativity', 'unlock', 'battle', 'solely']\n",
      "['enforceable', 'representativity', 'unlock', 'battle', 'solely']\n"
     ]
    }
   ],
   "source": [
    "for i in range(44):\n",
    "    print(cluster_terms[i][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is much less stable but enables to show that the sliding window allows more change in the topic meaning. However, we can see that the global topic meaning strongly changed.\n",
    "\n",
    "With w_size = 10, we get :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cluster_terms, open( \"wsize10_nogram.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_terms = pickle.load(open( \"wsize10_nogram.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['serb', 'namibian', 'geographical', 'farming', 'differ']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['punishing', 'frequency', 'perpetuity', 'sharon', 'disarray']\n",
      "['field', 'reopen', 'senegal', 'africanist', 'shackled']\n",
      "['field', 'reopen', 'senegal', 'africanist', 'perpetuate']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'perpetuate']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'perpetuate']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'perpetuate']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'elude']\n",
      "['field', 'africanist', 'reopen', 'senegal', 'elude']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'elude']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'elude']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'elude']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'affliction']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'painted']\n",
      "['field', 'reopen', 'africanist', 'senegal', 'painted']\n",
      "['field', 'reopen', 'africanist', 'painted', 'senegal']\n",
      "['field', 'reopen', 'africanist', 'painted', 'senegal']\n",
      "['field', 'reopen', 'africanist', 'painted', 'senegal']\n",
      "['field', 'reopen', 'africanist', 'painted', 'senegal']\n",
      "['field', 'reopen', 'africanist', 'painted', 'senegal']\n",
      "['field', 'reopen', 'painted', 'africanist', 'senegal']\n",
      "['field', 'reopen', 'painted', 'truth', 'senegal']\n",
      "['field', 'reopen', 'painted', 'truth', 'senegal']\n",
      "['field', 'reopen', 'painted', 'truth', 'senegal']\n",
      "['field', 'reopen', 'painted', 'truth', 'senegal']\n",
      "['field', 'reopen', 'painted', 'truth', 'senegal']\n",
      "['field', 'reopen', 'painted', 'truth', 'senegal']\n"
     ]
    }
   ],
   "source": [
    "for i in range(44):\n",
    "    print(cluster_terms[i][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynamicTopicModeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
