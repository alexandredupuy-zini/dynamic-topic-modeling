{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from kedro.context import load_context\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import matutils\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary, MmCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_20NG(df):\n",
    "    return df\n",
    "\n",
    "def get_data_20NG():\n",
    "    train_data = fetch_20newsgroups(subset='train')\n",
    "    test_data = fetch_20newsgroups(subset='test')\n",
    "    tmp_train = [train_data.data[doc] for doc in range(len(train_data.data))]\n",
    "    tmp_test = [test_data.data[doc] for doc in range(len(test_data.data))]\n",
    "    data = tmp_train + tmp_test\n",
    "    timestamps = [0] * len(data)\n",
    "    df = pd.DataFrame({'text':data, 'timestamp':timestamps})\n",
    "    df = preprocess_20NG(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_UNGD(df):\n",
    "    df.rename({'year':'timestamp'}, axis=1, inplace=True)\n",
    "    df = df[['timestamp', 'text']].copy()\n",
    "    return df\n",
    "\n",
    "def get_data_UNGD(id='1Gx1oBjcsJOgklxLGZ8iEoNJ5XHnCEcsm',\n",
    "                  destination='data/01_raw/un-general-debates.csv'):\n",
    "\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    if not os.path.isfile(destination):\n",
    "\n",
    "        URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "        session = requests.Session()\n",
    "        response = session.get(URL, params={'id': id}, stream=True)\n",
    "        token = get_confirm_token(response)\n",
    "\n",
    "        if token:\n",
    "            params = {'id': id, 'confirm': token}\n",
    "            response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "        print('Downloading dataset...')\n",
    "        save_response_content(response, destination)\n",
    "        print('Done downloading')\n",
    "\n",
    "    df = pd.read_csv(destination)\n",
    "    df = preprocess_UNGD(df) # apply specific pre-processing specific to UN dataset\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_SOGE(df):\n",
    "    df.rename({'Date':'timestamp'}, axis=1, inplace=True)\n",
    "    df.rename({'raisons_recommandation':'text'}, axis=1, inplace=True)\n",
    "    df = df[['timestamp', 'text']].copy()\n",
    "    return df\n",
    "\n",
    "def get_data_SOGE(data_path='data/01_raw/verbatims_soge.csv'):\n",
    "    df = pd.read_csv(data_path, sep=';', encoding='latin-1')\n",
    "    df = preprocess_SOGE(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset='20NG'):\n",
    "    if dataset == '20NG':\n",
    "        df = get_data_20NG()\n",
    "    elif dataset == 'UNGD':\n",
    "        df = get_data_UNGD()\n",
    "    elif dataset == 'SOGE':\n",
    "        df = get_data_SOGE()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data('20NG') #20NG, UNGD, SOGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_paragraph(docs, timestamps):\n",
    "    tmp_docs, tmp_timestamps = [], []\n",
    "    for i, doc in enumerate(docs):\n",
    "        splitted_doc = doc.split('.\\n')\n",
    "        for sd in splitted_doc:\n",
    "            tmp_docs.append(sd)\n",
    "            tmp_timestamps.append(timestamps[i])\n",
    "    return tmp_docs, tmp_timestamps\n",
    "\n",
    "def lowerize(docs):\n",
    "    # Convert to lowercase.\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = str(docs[idx]).lower()\n",
    "    return docs\n",
    "\n",
    "def tokenize(docs):\n",
    "    # Split into words.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])\n",
    "    return docs\n",
    "\n",
    "def remove_stop_words(docs):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = [w for w in docs[idx] if not w in stop_words]\n",
    "    return docs\n",
    "\n",
    "def remove_numbers(docs):\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    return docs\n",
    "\n",
    "def remove_word_with_length(docs, length=1):\n",
    "    # Remove words that are only (length=1) character.\n",
    "    docs = [[token for token in doc if len(token) > length] for doc in docs]\n",
    "    return docs\n",
    "\n",
    "def lemmatize(docs):\n",
    "    # Lemmatize the documents\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    return docs\n",
    "\n",
    "def add_bigram(docs, min_bigram_count=20):\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=min_bigram_count)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset:pd.DataFrame,\n",
    "                       flag_split_by_paragraph:bool,\n",
    "                       flag_lemmatize:bool,\n",
    "                       flag_bigram:bool,\n",
    "                       min_bigram_count:int,\n",
    "                       flag_word_analysis:bool) -> Dict[str, Any]:\n",
    "    t0 = time()\n",
    "\n",
    "    print('\\nCurrent set of parameters :\\n')\n",
    "    print('\\tflag_split_by_paragraph : {}'.format(flag_split_by_paragraph))\n",
    "    print('\\tflag_lemmatize : {}'.format(flag_lemmatize))\n",
    "    print('\\tflag_bigram : {}'.format(flag_bigram))\n",
    "    print('\\tmin_bigram_count : {}'.format(min_bigram_count))\n",
    "    #print('\\textreme_no_below : {}'.format(extreme_no_below))\n",
    "    #print('\\textreme_no_above : {}'.format(extreme_no_above))\n",
    "    print('\\tflag_word_analysis : {}\\n'.format(flag_word_analysis))\n",
    "\n",
    "    print('\\nStart preprocessing on dataset')\n",
    "\n",
    "    if \"text\" not in dataset.columns:\n",
    "        raise ValueError('Dataset does not have a column named \"text\". You must rename the your text column to \"text\".')\n",
    "    if \"timestamp\" not in dataset.columns:\n",
    "        raise ValueError('Dataset does not have a column named \"timestamp\". You must rename your time column to \"timestamp\".')\n",
    "\n",
    "    dataset.sort_values('timestamp', inplace=True)\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    docs = dataset['text'].values\n",
    "    timestamps = dataset['timestamp'].values\n",
    "\n",
    "    if flag_split_by_paragraph:\n",
    "        print('\\nSplitting by paragraph...')\n",
    "        docs, timestamps = split_by_paragraph(docs, timestamps)\n",
    "\n",
    "    print('\\nLowerizing...')\n",
    "    docs = lowerize(docs)\n",
    "\n",
    "    print('\\nTokenizing...')\n",
    "    docs = tokenize(docs)\n",
    "\n",
    "    if flag_bigram:\n",
    "        print('\\nAdding bigrams...')\n",
    "        docs = add_bigram(docs, min_bigram_count)\n",
    "\n",
    "    if flag_word_analysis:\n",
    "\n",
    "        print('\\nBasic word analysis enabled. It will take more time to compute......')\n",
    "\n",
    "        len_starting_vocab = len(Dictionary(docs))\n",
    "        print('\\nBeginning dictionary contains : {} words'.format(len_starting_vocab))\n",
    "\n",
    "        print('\\nRemoving stop words...')\n",
    "        docs = remove_stop_words(docs)\n",
    "        curr_len_vocab = len(Dictionary(docs))\n",
    "        len_rm_words = len_starting_vocab - curr_len_vocab\n",
    "        len_vocab = curr_len_vocab\n",
    "        freq = round(len_rm_words / len_starting_vocab, 3) * 100\n",
    "        print('\\tRemoved {} stopwords from dictionary. It represents {}% of total words in starting vocabulary'.format(len_rm_words, freq))\n",
    "        print('\\tCurrent length of the vocabulary:', len_vocab)\n",
    "        \n",
    "        print('\\nRemoving unique numbers (not words that contain numbers)...')\n",
    "        docs = remove_numbers(docs)\n",
    "        curr_len_vocab = len(Dictionary(docs))\n",
    "        len_rm_words = len_vocab - curr_len_vocab\n",
    "        len_vocab = curr_len_vocab\n",
    "        freq = round(len_rm_words / len_starting_vocab, 3) * 100\n",
    "        print('\\tRemoved {} numeric words from dictionary. It represents {}% of total words in starting vocabulary'.format(len_rm_words, freq))\n",
    "        print('\\tCurrent length of the vocabulary:', len_vocab)\n",
    "        \n",
    "        print('\\nRemoving words that contain only one character...')\n",
    "        docs = remove_word_with_length(docs, length=1)\n",
    "        curr_len_vocab = len(Dictionary(docs))\n",
    "        len_rm_words = len_vocab - curr_len_vocab\n",
    "        len_vocab = curr_len_vocab\n",
    "        freq = round(len_rm_words / len_starting_vocab, 3) * 100\n",
    "        print('\\tRemoved {} one length characters from dictionary. It represents {}% of total words in starting vocabulary'.format(len_rm_words, freq))\n",
    "        print('\\tCurrent length of the vocabulary:', len_vocab)\n",
    "        \n",
    "        if flag_lemmatize:\n",
    "            print('\\nLemmatizing...')\n",
    "            docs = lemmatize(docs)\n",
    "            curr_len_vocab = len(Dictionary(docs))\n",
    "            len_rm_words = len_vocab - curr_len_vocab\n",
    "            len_vocab = curr_len_vocab\n",
    "            freq = round(len_rm_words / len_starting_vocab, 3) * 100\n",
    "            print('\\tRemoved {} words from dictionary. It represents {}% of total words in starting vocabulary'.format(len_rm_words, freq))\n",
    "            print('\\tCurrent length of the vocabulary:', len_vocab)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        print('\\nBasic word analysis disabled.')\n",
    "\n",
    "        print('\\nRemoving stop words...')\n",
    "        docs = remove_stop_words(docs)\n",
    "\n",
    "        print('\\nRemoving unique numbers (not words that contain numbers)...')\n",
    "        docs = remove_numbers(docs)\n",
    "\n",
    "        print('\\nRemoving words that contain only one character...')\n",
    "        docs = remove_word_with_length(docs, length=1)\n",
    "        \n",
    "        if flag_lemmatize:\n",
    "            print('\\nLemmatizing...')\n",
    "            docs = lemmatize(docs)\n",
    "\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    #print('\\nCreating dictionary...')\n",
    "    #dictionary = Dictionary(docs)\n",
    "\n",
    "    #print('\\nFiltering extremes...')\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    #dictionary.filter_extremes(no_below=extreme_no_below, no_above=extreme_no_above)\n",
    "    #extreme_no_below_str = str(extreme_no_below) if extreme_no_below > 1 else str(extreme_no_below*100)+'%'\n",
    "    #extreme_no_above_str = str(extreme_no_above) if extreme_no_above > 1 else str(extreme_no_above*100)+'%'\n",
    "    #print('\\tKeeping words in no less than {} documents & in no more than {} documents'.format(extreme_no_below_str, extreme_no_above_str))\n",
    "\n",
    "    # Bag-of-words representation of the documents.\n",
    "    #corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    #print('Number of unique tokens: %d' % len(dictionary))\n",
    "    #print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "\n",
    "    print('\\nTimestamps processing...')\n",
    "    unique_time = np.unique(timestamps)\n",
    "    mapper_time = dict(zip(unique_time, range(len(unique_time))))\n",
    "    timestamps = np.array([mapper_time[timestamps[i]] for i in range(len(timestamps))])\n",
    "    \n",
    "\n",
    "    print('\\nDone in {} minutes'.format(int((time()-t0)/60)))\n",
    "\n",
    "    return [\n",
    "        docs,\n",
    "        #corpus,\n",
    "        timestamps,\n",
    "        #dictionary\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current set of parameters :\n",
      "\n",
      "\tflag_split_by_paragraph : False\n",
      "\tflag_lemmatize : True\n",
      "\tflag_bigram : False\n",
      "\tmin_bigram_count : 0\n",
      "\tflag_word_analysis : True\n",
      "\n",
      "\n",
      "Start preprocessing on dataset\n",
      "\n",
      "Lowerizing...\n",
      "\n",
      "Tokenizing...\n",
      "\n",
      "Basic word analysis enabled. It will take more time to compute......\n",
      "2020-04-03 10:28:53,638 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-03 10:28:56,008 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary(118331 unique tokens: ['15', '2', '60s', '70s', 'a']...)\n",
      "2020-04-03 10:28:58,158 - gensim.corpora.dictionary - INFO - built Dictionary(173807 unique tokens: ['15', '2', '60s', '70s', 'a']...) from 18846 documents (total 5986471 corpus positions)\n",
      "\n",
      "Beginning dictionary contains : 173807 words\n",
      "\n",
      "Removing stop words...\n",
      "2020-04-03 10:28:58,474 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-03 10:29:00,381 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary(118179 unique tokens: ['15', '2', '60s', '70s', 'addition']...)\n",
      "2020-04-03 10:29:01,966 - gensim.corpora.dictionary - INFO - built Dictionary(173655 unique tokens: ['15', '2', '60s', '70s', 'addition']...) from 18846 documents (total 3696319 corpus positions)\n",
      "\tRemoved 152 stopwords from dictionary. It represents 0.1% of total words in starting vocabulary\n",
      "\tCurrent length of the vocabulary: 173655\n",
      "\n",
      "Removing unique numbers (not words that contain numbers)...\n",
      "2020-04-03 10:29:02,299 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-03 10:29:03,878 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary(108781 unique tokens: ['60s', '70s', 'addition', 'anyone', 'body']...)\n",
      "2020-04-03 10:29:05,337 - gensim.corpora.dictionary - INFO - built Dictionary(159881 unique tokens: ['60s', '70s', 'addition', 'anyone', 'body']...) from 18846 documents (total 3412146 corpus positions)\n",
      "\tRemoved 13774 numeric words from dictionary. It represents 7.9% of total words in starting vocabulary\n",
      "\tCurrent length of the vocabulary: 159881\n",
      "\n",
      "Removing words that contain only one character...\n",
      "2020-04-03 10:29:05,687 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-03 10:29:07,420 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary(108758 unique tokens: ['60s', '70s', 'addition', 'anyone', 'body']...)\n",
      "2020-04-03 10:29:08,901 - gensim.corpora.dictionary - INFO - built Dictionary(159856 unique tokens: ['60s', '70s', 'addition', 'anyone', 'body']...) from 18846 documents (total 3302129 corpus positions)\n",
      "\tRemoved 25 one length characters from dictionary. It represents 0.0% of total words in starting vocabulary\n",
      "\tCurrent length of the vocabulary: 159856\n",
      "\n",
      "Lemmatizing...\n",
      "2020-04-03 10:29:19,415 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-03 10:29:20,978 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary(102988 unique tokens: ['60', '70', 'addition', 'anyone', 'body']...)\n",
      "2020-04-03 10:29:22,411 - gensim.corpora.dictionary - INFO - built Dictionary(152643 unique tokens: ['60', '70', 'addition', 'anyone', 'body']...) from 18846 documents (total 3302129 corpus positions)\n",
      "\tRemoved 7213 words from dictionary. It represents 4.2% of total words in starting vocabulary\n",
      "\tCurrent length of the vocabulary: 152643\n",
      "\n",
      "Timestamps processing...\n",
      "\n",
      "Done in 0 minutes\n"
     ]
    }
   ],
   "source": [
    "docs, timestamps = preprocess_dataset(df, flag_split_by_paragraph=False, flag_lemmatize=True, flag_bigram=False,\n",
    "                                      min_bigram_count=0, flag_word_analysis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vocab(docs, vocab):\n",
    "    docs = np.array([[w for w in doc if w in vocab] for doc in docs])\n",
    "    return docs\n",
    "\n",
    "def remove_empty(docs, timestamps):\n",
    "    tmp_docs = []\n",
    "    tmp_timestamps = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        if(doc != []):\n",
    "            tmp_docs.append(doc)\n",
    "            tmp_timestamps.append(timestamps[i])\n",
    "    return tmp_docs, tmp_timestamps\n",
    "\n",
    "def remove_by_threshold(docs, timestamps, threshold):\n",
    "    tmp_docs = []\n",
    "    tmp_timestamps = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        if(len(doc) > threshold):\n",
    "            tmp_docs.append(doc)\n",
    "            tmp_timestamps.append(timestamps[i])\n",
    "    return tmp_docs, tmp_timestamps\n",
    "\n",
    "def convert_to_bow(docs, dictionary):\n",
    "    return [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "def create_list_words(in_docs, vocab):\n",
    "    return [vocab.token2id[x] for y in in_docs for x in y]\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "def create_sparse(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices), (doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "def create_sparse_matrix(docs, vocab):\n",
    "    len_vocab = len(vocab)\n",
    "\n",
    "    # Getting lists of words and doc_indices\n",
    "    words = create_list_words(docs, vocab)\n",
    "\n",
    "    # Get doc indices\n",
    "    doc_indices = create_doc_indices(docs)\n",
    "    \n",
    "    n_docs = len(docs)\n",
    "\n",
    "    # Create bow representation\n",
    "    bow = create_sparse(doc_indices, words, n_docs, len_vocab)\n",
    "\n",
    "    # Split bow intro token/value pairs\n",
    "    #print('splitting bow intro token/value pairs and saving to disk...')\n",
    "    #bow_tokens, bow_counts = split_bow(bow, n_docs)\n",
    "\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(docs:np.array,\n",
    "               #corpus:MmCorpus,\n",
    "               timestamps:np.array,\n",
    "               #dictionary:Dictionary,\n",
    "               extreme_no_below,\n",
    "               extreme_no_above,\n",
    "               test_size:float,\n",
    "               val_size:float) -> Dict[str,Any]:\n",
    "\n",
    "    # Split indexes into train/val/test sets\n",
    "    print('\\nSplitting indexes into train/val/test')\n",
    "    num_docs = len(docs)\n",
    "\n",
    "    val_len = int(val_size * num_docs)\n",
    "    test_len = int(test_size * num_docs)\n",
    "    train_len = int(num_docs - val_len - test_len)\n",
    "\n",
    "    idx_permute = np.random.permutation(num_docs).astype(int)\n",
    "\n",
    "    # Split docs and timestamps into train/val/test sets\n",
    "    print('\\nSpliiting docs and timestamps into train/val/test')\n",
    "    train_docs = [docs[idx_permute[i]] for i in range(train_len)]\n",
    "    val_docs = [docs[idx_permute[train_len+i]] for i in range(val_len)]\n",
    "    test_docs = [docs[idx_permute[train_len+val_len+i]] for i in range(test_len)]\n",
    "\n",
    "    train_timestamps = [timestamps[idx_permute[i]] for i in range(train_len)]\n",
    "    val_timestamps = [timestamps[idx_permute[train_len+i]] for i in range(val_len)]\n",
    "    test_timestamps = [timestamps[idx_permute[train_len+val_len+i]] for i in range(test_len)]\n",
    "\n",
    "    print('\\tNumber of documents in train set : {} [this should be equal to {} and {}]'.format(len(train_docs), train_len, len(train_timestamps)))\n",
    "    print('\\tNumber of documents in test set : {} [this should be equal to {} and {}]'.format(len(test_docs), test_len, len(test_timestamps)))\n",
    "    print('\\tNumber of documents in validation set: {} [this should be equal to {} and {}]'.format(len(val_docs), val_len, len(val_timestamps)))\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    print('\\nCreating dictionary...')\n",
    "    train_dictionary = Dictionary(train_docs)\n",
    "\n",
    "    print('\\tFiltering extremes...')\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    train_dictionary.filter_extremes(no_below=extreme_no_below, no_above=extreme_no_above)\n",
    "    extreme_no_below_str = str(extreme_no_below) if extreme_no_below > 1 else str(extreme_no_below*100)+'%'\n",
    "    extreme_no_above_str = str(extreme_no_above) if extreme_no_above > 1 else str(extreme_no_above*100)+'%'\n",
    "    print('\\tKeeping words in no less than {} documents & in no more than {} documents'.format(extreme_no_below_str, extreme_no_above_str))\n",
    "    print('\\tNumber of unique tokens: %d' % len(train_dictionary))\n",
    "\n",
    "    # Remove words not in train_data\n",
    "    print('\\nRemoving words not in train data .....')\n",
    "    train_vocab = train_dictionary.token2id\n",
    "    train_docs = remove_vocab(train_docs, train_vocab)\n",
    "    val_docs = remove_vocab(val_docs, train_vocab)\n",
    "    test_docs = remove_vocab(test_docs, train_vocab)\n",
    "\n",
    "    # Remove empty documents\n",
    "    print('\\nRemoving empty documents')\n",
    "    train_docs, train_timestamps = remove_empty(train_docs, train_timestamps)\n",
    "    test_docs, test_timestamps = remove_empty(test_docs, test_timestamps)\n",
    "    val_docs, val_timestamps = remove_empty(val_docs, val_timestamps)\n",
    "\n",
    "    # Remove test documents with length=1\n",
    "    print('\\nRemoving test documents with length 1')\n",
    "    test_docs, test_timestamps = remove_by_threshold(test_docs, test_timestamps, 1)\n",
    "\n",
    "    # Split test set in 2 halves\n",
    "    print('\\nSplitting test set in 2 halves')\n",
    "    test_docs_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in test_docs]\n",
    "    test_docs_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in test_docs]\n",
    "\n",
    "    # Convert to Bag-of-Words representation\n",
    "    print('\\nCreating bow representation...')\n",
    "    train_corpus = convert_to_bow(train_docs, train_dictionary)\n",
    "    val_corpus = convert_to_bow(val_docs, train_dictionary)\n",
    "    test_corpus = convert_to_bow(test_docs, train_dictionary)\n",
    "\n",
    "    test_corpus_h1 = convert_to_bow(test_docs_h1, train_dictionary)\n",
    "    test_corpus_h2 = convert_to_bow(test_docs_h2, train_dictionary)\n",
    "    \n",
    "    print('\\tTrain bag of words shape : {}'.format(len(train_corpus)))\n",
    "    print('\\tVal bag of words shape : {}'.format(len(val_corpus)))\n",
    "    print('\\tTest bag of words shape : {}'.format(len(test_corpus)))\n",
    "    print('\\tTest set 1 bag of words shape : {}'.format(len(test_corpus_h1)))\n",
    "    print('\\tTest set 2 bag of words shape : {}'.format(len(test_corpus_h2)))\n",
    "\n",
    "    # Convert to sparse matrices (scipy COO sparse matrix)\n",
    "    print('\\nCreating sparse matrices')\n",
    "    train_sparse = create_sparse_matrix(train_docs, train_dictionary)\n",
    "    test_sparse = create_sparse_matrix(test_docs, train_dictionary)\n",
    "    test_sparse_h1 = create_sparse_matrix(test_docs_h1, train_dictionary)\n",
    "    test_sparse_h2 = create_sparse_matrix(test_docs_h2, train_dictionary)\n",
    "    val_sparse = create_sparse_matrix(val_docs, train_dictionary)\n",
    "\n",
    "    print('\\nDone splitting data.')\n",
    "\n",
    "    return dict(\n",
    "        train_docs=train_docs, train_corpus=train_corpus, train_sparse=train_sparse,\n",
    "        val_docs=val_docs, val_corpus=val_corpus, val_sparse=val_sparse,\n",
    "        test_docs=test_docs, test_corpus=test_corpus, test_sparse=test_sparse,\n",
    "        test_docs_h1=test_docs_h1, test_corpus_h1=test_corpus_h1, test_sparse_h1=test_sparse_h1,\n",
    "        test_docs_h2=test_docs_h2, test_corpus_h2=test_corpus_h2, test_sparse_h2=test_sparse_h2,\n",
    "        train_timestamps=train_timestamps, val_timestamps=val_timestamps, test_timestamps=test_timestamps,\n",
    "        dictionary=train_dictionary\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting indexes into train/val/test\n",
      "\n",
      "Spliiting docs and timestamps into train/val/test\n",
      "\tNumber of documents in train set : 15078 [this should be equal to 15078 and 15078]\n",
      "\tNumber of documents in test set : 942 [this should be equal to 942 and 942]\n",
      "\tNumber of documents in validation set: 2826 [this should be equal to 2826 and 2826]\n",
      "\n",
      "Creating dictionary...\n",
      "2020-04-03 10:35:10,790 - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-03 10:35:12,400 - gensim.corpora.dictionary - INFO - adding document #10000 to Dictionary(100923 unique tokens: ['age', 'also', 'andrew', 'audiophile', 'call']...)\n",
      "2020-04-03 10:35:13,305 - gensim.corpora.dictionary - INFO - built Dictionary(127065 unique tokens: ['age', 'also', 'andrew', 'audiophile', 'call']...) from 15078 documents (total 2622559 corpus positions)\n",
      "\tFiltering extremes...\n",
      "2020-04-03 10:35:13,444 - gensim.corpora.dictionary - INFO - discarding 124350 tokens: [('audiophile', 7), ('chan', 29), ('forte', 11), ('klipsch', 3), ('line', 15035), ('mint', 43), ('organization', 14521), ('packagaing', 2), ('pc1o', 4), ('po3', 18)]...\n",
      "2020-04-03 10:35:13,444 - gensim.corpora.dictionary - INFO - keeping 2715 tokens which were in no less than 100 and no more than 10554 (=70.0%) documents\n",
      "2020-04-03 10:35:13,487 - gensim.corpora.dictionary - INFO - resulting dictionary: Dictionary(2715 unique tokens: ['age', 'also', 'andrew', 'call', 'carnegie']...)\n",
      "\tKeeping words in no less than 100 documents & in no more than 70.0% documents\n",
      "\tNumber of unique tokens: 2715\n",
      "\n",
      "Removing words not in train data .....\n",
      "\n",
      "Removing empty documents\n",
      "\n",
      "Removing test documents with length 1\n",
      "\n",
      "Splitting test set in 2 halves\n",
      "\n",
      "Creating bow representation...\n",
      "\tTrain bag of words shape : 15078\n",
      "\tVal bag of words shape : 2826\n",
      "\tTest bag of words shape : 942\n",
      "\tTest set 1 bag of words shape : 942\n",
      "\tTest set 2 bag of words shape : 942\n",
      "\n",
      "Creating sparse matrices\n",
      "\n",
      "Done splitting data.\n"
     ]
    }
   ],
   "source": [
    "res = split_data(docs, timestamps, extreme_no_below=100, extreme_no_above=0.70, test_size=0.05, val_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DynamicTopicModeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
